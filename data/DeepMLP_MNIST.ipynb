{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the MNIST handwritten digit recognition task\n",
    "Based on [https://cambridgespark.com/content/tutorials/deep-learning-for-complete-beginners-recognising-handwritten-digits/index.html]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "from keras.datasets import mnist # subroutines for fetching the MNIST dataset\n",
    "from keras.models import Model # basic class for specifying and training a neural network\n",
    "from keras.layers import Input, Dense # the two types of neural network layer we will be using\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "### Define some parameters of our model. They are assumed to be fixed before training starts.\n",
    "\n",
    "- The batch size, representing the number of training examples being used simultaneously during a single iteration of the gradient descent algorithm \n",
    "- The number of epochs, representing the number of times the training algorithm will iterate over the entire training set before terminating \n",
    "- The number of neurons in each of the two hidden layers of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # in each iteration, we consider 128 training examples at once\n",
    "num_epochs = 20 # we iterate twenty times over the entire training set\n",
    "hidden_size = 512 # there will be 512 neurons in both hidden layers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the input data, we will first flatten the images into 1D (as we will consider each pixel as a separate input feature), and we will then force the pixel intensity values to be in the [0,1] range by dividing them by 255. \n",
    "\n",
    "## Probabilistic classification\n",
    "### Outputting a value which corresponds to the probability of the input being of that particular class.\n",
    "\n",
    "- This implies a need to transform the training output data into a \"one-hot\" encoding: for example, if the desired output class is 3, and there are five classes overall (labelled 0 to 4), then an appropriate one-hot encoding is: [0 0 0 1 0]. Keras, once again, provides us with an out-of-the-box functionality for doing just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 8s 1us/step\n"
     ]
    }
   ],
   "source": [
    "num_train = 60000 # there are 60000 training examples in MNIST\n",
    "num_test = 10000 # there are 10000 test examples in MNIST\n",
    "\n",
    "height, width, depth = 28, 28, 1 # MNIST images are 28x28 and greyscale\n",
    "num_classes = 10 # there are 10 classes (1 per digit)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # fetch MNIST data\n",
    "\n",
    "X_train = X_train.reshape(num_train, height * width) # Flatten data to 1D\n",
    "X_test = X_test.reshape(num_test, height * width) # Flatten data to 1D\n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255 # Normalise data to [0, 1] range\n",
    "X_test /= 255 # Normalise data to [0, 1] range\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our model!\n",
    "### Using a stack of three Dense layers, which correspond to a fully unrestricted MLP structure\n",
    "\n",
    "- Use ReLU activations for the neurons in the first two layers, and a softmax activation for the neurons in the final one. This activation is designed to turn any real-valued vector into a vector of probabilities, and is defined as follows, for the j-th neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An excellent feature of Keras, that sets it apart from frameworks such as TensorFlow, is automatic inference of shapes; we only need to specify the shape of the input layer, and afterwards Keras will take care of initialising the weight variables with proper shapes. Once all the layers have been defined, we simply need to identify the input(s) and the output(s) in order to define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(height * width,)) # Our input is a 1D vector of size 784 (= 28*28)\n",
    "hidden_1 = Dense(hidden_size, activation='relu')(inp) # First hidden ReLU layer\n",
    "hidden_2 = Dense(hidden_size, activation='relu')(hidden_1) # Second hidden ReLU layer\n",
    "out = Dense(num_classes, activation='softmax')(hidden_2) # Output softmax layer\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish off specifying the model, we need to define our loss function, the optimisation algorithm to use, and which metrics to report.\n",
    "\n",
    "## Cross-entropy loss\n",
    "### This loss is better for probabilistic tasks (i.e. ones with logistic/softmax output neurons)\n",
    "- its manner of derivation – it aims only to maximise the model's confidence in the correct class\n",
    "- not concerned with the distribution of probabilities for other classes (while the squared error loss would dedicate equal attention to getting all of the other class probabilities as close to zero as possible). This is due to the fact that incorrect classes, i.e. classes i′ with ŷ i′=0, eliminate the respective neuron's output from the loss function.\n",
    "\n",
    "## Optimisation algorithm\n",
    "### Revolve around some form of gradient descent\n",
    "- their key differences revolve around the manner in which the previously mentioned learning rate, η, is chosen or adapted during training. \n",
    "- here we will use the Adam optimiser, which typically performs well.\n",
    "\n",
    "## Accuracy\n",
    "### the proportion of the inputs classified correctly.\n",
    "- as our classes are balanced (there is an equal amount of handwritten digits across all ten classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training algorithm\n",
    "### Finally, we call the training algorithm with the determined batch size and epoch count. \n",
    "- It is good practice to set aside a fraction of the training data to be used just for verification that our algorithm is (still) properly generalising (this is commonly referred to as the validation set); here we will hold out 10% of the data for this purpose.\n",
    "\n",
    "- An excellent out-of-the-box feature of Keras is verbosity; it's able to provide detailed real-time pretty-printing of the training algorithm's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "54000/54000 [==============================] - 24s 437us/step - loss: 0.2104 - acc: 0.9378 - val_loss: 0.0989 - val_acc: 0.9718\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 23s 427us/step - loss: 0.0788 - acc: 0.9758 - val_loss: 0.0766 - val_acc: 0.9753\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 23s 425us/step - loss: 0.0502 - acc: 0.9838 - val_loss: 0.0572 - val_acc: 0.9828\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 26s 475us/step - loss: 0.0340 - acc: 0.9886 - val_loss: 0.0709 - val_acc: 0.9798\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 25s 472us/step - loss: 0.0279 - acc: 0.9906 - val_loss: 0.0738 - val_acc: 0.9790\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 25s 459us/step - loss: 0.0234 - acc: 0.9922 - val_loss: 0.0677 - val_acc: 0.9838\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 25s 464us/step - loss: 0.0188 - acc: 0.9935 - val_loss: 0.0721 - val_acc: 0.9828\n",
      "Epoch 8/20\n",
      "54000/54000 [==============================] - 25s 458us/step - loss: 0.0176 - acc: 0.9938 - val_loss: 0.0849 - val_acc: 0.9788\n",
      "Epoch 9/20\n",
      "54000/54000 [==============================] - 26s 476us/step - loss: 0.0171 - acc: 0.9944 - val_loss: 0.0683 - val_acc: 0.9840\n",
      "Epoch 10/20\n",
      "54000/54000 [==============================] - 26s 478us/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0825 - val_acc: 0.9810\n",
      "Epoch 11/20\n",
      "54000/54000 [==============================] - 28s 520us/step - loss: 0.0132 - acc: 0.9958 - val_loss: 0.1188 - val_acc: 0.9762\n",
      "Epoch 12/20\n",
      "54000/54000 [==============================] - 26s 480us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.0879 - val_acc: 0.9817\n",
      "Epoch 13/20\n",
      "54000/54000 [==============================] - 27s 505us/step - loss: 0.0096 - acc: 0.9967 - val_loss: 0.0939 - val_acc: 0.9812\n",
      "Epoch 14/20\n",
      "54000/54000 [==============================] - 35s 644us/step - loss: 0.0123 - acc: 0.9963 - val_loss: 0.0953 - val_acc: 0.9812\n",
      "Epoch 15/20\n",
      "54000/54000 [==============================] - 37s 691us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.1104 - val_acc: 0.9787\n",
      "Epoch 16/20\n",
      "54000/54000 [==============================] - 28s 527us/step - loss: 0.0118 - acc: 0.9964 - val_loss: 0.1238 - val_acc: 0.9765\n",
      "Epoch 17/20\n",
      "54000/54000 [==============================] - 26s 486us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0951 - val_acc: 0.9823\n",
      "Epoch 18/20\n",
      "54000/54000 [==============================] - 27s 492us/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0856 - val_acc: 0.9842\n",
      "Epoch 19/20\n",
      "54000/54000 [==============================] - 26s 484us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.1130 - val_acc: 0.9827\n",
      "Epoch 20/20\n",
      "54000/54000 [==============================] - 26s 488us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.1011 - val_acc: 0.9842\n",
      "10000/10000 [==============================] - 2s 201us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10260707112895712, 0.9812]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, epochs=num_epochs,\n",
    "          verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "model.evaluate(X_test, Y_test, verbose=1) # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, our model achieves an accuracy of 98.55% on the test set; this is quite respectable for such a simple model, despite being outclassed by state-of-the-art approaches enumerated here.\n",
    "\n",
    "## Attempt different hyperparameter values/optimisation algorithms/activation functions, add more hidden layers, etc. \n",
    "### To achieve accuracies above 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "- 그냥 네우론의 숫자를 늘리는것은 도움이 되지 않는다.\n",
    "예) 512개에서 800개로 늘렸더니 시간이 더 걸리고 accuracy가 98.55% 에서 98.12% 로 줄어듬.\n",
    "- 네우론 수를 512개로 두고 레이어를 하나 더 늘림. 98.24% 나옴.\n",
    "- 베치 사이즈를 두배로 늘리고 3개의 히든 레이어로 하니, 98.13%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "54000/54000 [==============================] - 9s 171us/step - loss: 0.2764 - acc: 0.9212 - val_loss: 0.1128 - val_acc: 0.9677\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 9s 175us/step - loss: 0.0967 - acc: 0.9708 - val_loss: 0.0803 - val_acc: 0.9768\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 8s 145us/step - loss: 0.0588 - acc: 0.9817 - val_loss: 0.0746 - val_acc: 0.9798\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 8s 141us/step - loss: 0.0386 - acc: 0.9883 - val_loss: 0.0791 - val_acc: 0.9768\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 8s 153us/step - loss: 0.0282 - acc: 0.9909 - val_loss: 0.0761 - val_acc: 0.9798\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 8s 146us/step - loss: 0.0228 - acc: 0.9929 - val_loss: 0.0677 - val_acc: 0.9807\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 8s 144us/step - loss: 0.0173 - acc: 0.9944 - val_loss: 0.1029 - val_acc: 0.9727\n",
      "Epoch 8/20\n",
      "54000/54000 [==============================] - 8s 146us/step - loss: 0.0124 - acc: 0.9961 - val_loss: 0.0749 - val_acc: 0.9795\n",
      "Epoch 9/20\n",
      "54000/54000 [==============================] - 8s 148us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0856 - val_acc: 0.9798\n",
      "Epoch 10/20\n",
      "54000/54000 [==============================] - 8s 146us/step - loss: 0.0098 - acc: 0.9966 - val_loss: 0.0797 - val_acc: 0.9802\n",
      "Epoch 11/20\n",
      "54000/54000 [==============================] - 8s 146us/step - loss: 0.0071 - acc: 0.9977 - val_loss: 0.0777 - val_acc: 0.9833\n",
      "Epoch 12/20\n",
      "54000/54000 [==============================] - 8s 149us/step - loss: 0.0119 - acc: 0.9959 - val_loss: 0.0917 - val_acc: 0.9813\n",
      "Epoch 13/20\n",
      "54000/54000 [==============================] - 8s 148us/step - loss: 0.0136 - acc: 0.9951 - val_loss: 0.0874 - val_acc: 0.9798\n",
      "Epoch 14/20\n",
      "54000/54000 [==============================] - 8s 149us/step - loss: 0.0095 - acc: 0.9967 - val_loss: 0.0808 - val_acc: 0.9823\n",
      "Epoch 15/20\n",
      "54000/54000 [==============================] - 8s 149us/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0874 - val_acc: 0.9822\n",
      "Epoch 16/20\n",
      "54000/54000 [==============================] - 9s 157us/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0828 - val_acc: 0.9825\n",
      "Epoch 17/20\n",
      "54000/54000 [==============================] - 8s 154us/step - loss: 0.0072 - acc: 0.9973 - val_loss: 0.0887 - val_acc: 0.9812\n",
      "Epoch 18/20\n",
      "54000/54000 [==============================] - 8s 151us/step - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0982 - val_acc: 0.9805\n",
      "Epoch 19/20\n",
      "54000/54000 [==============================] - 8s 152us/step - loss: 0.0040 - acc: 0.9986 - val_loss: 0.0869 - val_acc: 0.9833\n",
      "Epoch 20/20\n",
      "54000/54000 [==============================] - 9s 167us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.1105 - val_acc: 0.9778\n",
      "10000/10000 [==============================] - 1s 126us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08921291756161713, 0.9813]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128*2 # in each iteration, we consider 128 training examples at once\n",
    "num_epochs = 20 # we iterate twenty times over the entire training set\n",
    "hidden_size = 512 # there will be 512 neurons in both hidden layers \n",
    "\n",
    "inp = Input(shape=(height * width,)) # Our input is a 1D vector of size 784 (= 28*28)\n",
    "hidden_1 = Dense(hidden_size, activation='relu')(inp) # First hidden ReLU layer\n",
    "hidden_2 = Dense(hidden_size, activation='relu')(hidden_1) # Second hidden ReLU layer\n",
    "hidden_3 = Dense(hidden_size, activation='relu')(hidden_2) # Third hidden ReLU layer\n",
    "out = Dense(num_classes, activation='softmax')(hidden_2) # Output softmax layer\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers\n",
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "model.fit(X_train, Y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, epochs=num_epochs,\n",
    "          verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "model.evaluate(X_test, Y_test, verbose=1) # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
