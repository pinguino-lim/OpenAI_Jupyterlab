{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the MNIST handwritten digit recognition task\n",
    "Based on [https://cambridgespark.com/content/tutorials/deep-learning-for-complete-beginners-recognising-handwritten-digits/index.html]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import tensorflow as tf\n",
    "from keras.datasets import mnist # subroutines for fetching the MNIST dataset\n",
    "from keras.models import Model, Sequential # basic class for specifying and training a neural network\n",
    "from keras.layers import Input, Dense # the two types of neural network layer we will be using\n",
    "from keras.utils import np_utils # utilities for one-hot encoding of ground truth values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "### Define some parameters of our model. They are assumed to be fixed before training starts.\n",
    "\n",
    "- The batch size, representing the number of training examples being used simultaneously during a single iteration of the gradient descent algorithm \n",
    "- The number of epochs, representing the number of times the training algorithm will iterate over the entire training set before terminating \n",
    "- The number of neurons in each of the two hidden layers of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # in each iteration, we consider 128 training examples at once\n",
    "num_epochs = 20 # we iterate twenty times over the entire training set\n",
    "hidden_size = 512 # there will be 512 neurons in both hidden layers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the input data, we will first flatten the images into 1D (as we will consider each pixel as a separate input feature), and we will then force the pixel intensity values to be in the [0,1] range by dividing them by 255. \n",
    "\n",
    "## Probabilistic classification\n",
    "### Outputting a value which corresponds to the probability of the input being of that particular class.\n",
    "\n",
    "- This implies a need to transform the training output data into a \"one-hot\" encoding: for example, if the desired output class is 3, and there are five classes overall (labelled 0 to 4), then an appropriate one-hot encoding is: [0 0 0 1 0]. Keras, once again, provides us with an out-of-the-box functionality for doing just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 60000 # there are 60000 training examples in MNIST\n",
    "num_test = 10000 # there are 10000 test examples in MNIST\n",
    "\n",
    "height, width, depth = 28, 28, 1 # MNIST images are 28x28 and greyscale\n",
    "num_classes = 10 # there are 10 classes (1 per digit)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # fetch MNIST data\n",
    "\n",
    "X_train = X_train.reshape(num_train, height * width) # Flatten data to 1D\n",
    "X_test = X_test.reshape(num_test, height * width) # Flatten data to 1D\n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255 # Normalise data to [0, 1] range\n",
    "X_test /= 255 # Normalise data to [0, 1] range\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels\n",
    "Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our model!\n",
    "### Using a stack of three Dense layers, which correspond to a fully unrestricted MLP structure\n",
    "\n",
    "- Use ReLU activations for the neurons in the first two layers, and a softmax activation for the neurons in the final one. This activation is designed to turn any real-valued vector into a vector of probabilities, and is defined as follows, for the j-th neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An excellent feature of Keras, that sets it apart from frameworks such as TensorFlow, is automatic inference of shapes; we only need to specify the shape of the input layer, and afterwards Keras will take care of initialising the weight variables with proper shapes. Once all the layers have been defined, we simply need to identify the input(s) and the output(s) in order to define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(height * width,)) # Our input is a 1D vector of size 784 (= 28*28)\n",
    "hidden_1 = Dense(hidden_size, activation='relu')(inp) # First hidden ReLU layer\n",
    "hidden_2 = Dense(hidden_size, activation='relu')(hidden_1) # Second hidden ReLU layer\n",
    "out = Dense(num_classes, activation='softmax')(hidden_2) # Output softmax layer\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish off specifying the model, we need to define our loss function, the optimisation algorithm to use, and which metrics to report.\n",
    "\n",
    "## Cross-entropy loss\n",
    "### This loss is better for probabilistic tasks (i.e. ones with logistic/softmax output neurons)\n",
    "- its manner of derivation – it aims only to maximise the model's confidence in the correct class\n",
    "- not concerned with the distribution of probabilities for other classes (while the squared error loss would dedicate equal attention to getting all of the other class probabilities as close to zero as possible). This is due to the fact that incorrect classes, i.e. classes i′ with ŷ i′=0, eliminate the respective neuron's output from the loss function.\n",
    "\n",
    "## Optimisation algorithm\n",
    "### Revolve around some form of gradient descent\n",
    "- their key differences revolve around the manner in which the previously mentioned learning rate, η, is chosen or adapted during training. \n",
    "- here we will use the Adam optimiser, which typically performs well.\n",
    "\n",
    "## Accuracy\n",
    "### the proportion of the inputs classified correctly.\n",
    "- as our classes are balanced (there is an equal amount of handwritten digits across all ten classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training algorithm\n",
    "### Finally, we call the training algorithm with the determined batch size and epoch count. \n",
    "- It is good practice to set aside a fraction of the training data to be used just for verification that our algorithm is (still) properly generalising (this is commonly referred to as the validation set); here we will hold out 10% of the data for this purpose.\n",
    "\n",
    "- An excellent out-of-the-box feature of Keras is verbosity; it's able to provide detailed real-time pretty-printing of the training algorithm's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "54000/54000 [==============================] - 24s 437us/step - loss: 0.2104 - acc: 0.9378 - val_loss: 0.0989 - val_acc: 0.9718\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 23s 427us/step - loss: 0.0788 - acc: 0.9758 - val_loss: 0.0766 - val_acc: 0.9753\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 23s 425us/step - loss: 0.0502 - acc: 0.9838 - val_loss: 0.0572 - val_acc: 0.9828\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 26s 475us/step - loss: 0.0340 - acc: 0.9886 - val_loss: 0.0709 - val_acc: 0.9798\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 25s 472us/step - loss: 0.0279 - acc: 0.9906 - val_loss: 0.0738 - val_acc: 0.9790\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 25s 459us/step - loss: 0.0234 - acc: 0.9922 - val_loss: 0.0677 - val_acc: 0.9838\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 25s 464us/step - loss: 0.0188 - acc: 0.9935 - val_loss: 0.0721 - val_acc: 0.9828\n",
      "Epoch 8/20\n",
      "54000/54000 [==============================] - 25s 458us/step - loss: 0.0176 - acc: 0.9938 - val_loss: 0.0849 - val_acc: 0.9788\n",
      "Epoch 9/20\n",
      "54000/54000 [==============================] - 26s 476us/step - loss: 0.0171 - acc: 0.9944 - val_loss: 0.0683 - val_acc: 0.9840\n",
      "Epoch 10/20\n",
      "54000/54000 [==============================] - 26s 478us/step - loss: 0.0114 - acc: 0.9965 - val_loss: 0.0825 - val_acc: 0.9810\n",
      "Epoch 11/20\n",
      "54000/54000 [==============================] - 28s 520us/step - loss: 0.0132 - acc: 0.9958 - val_loss: 0.1188 - val_acc: 0.9762\n",
      "Epoch 12/20\n",
      "54000/54000 [==============================] - 26s 480us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.0879 - val_acc: 0.9817\n",
      "Epoch 13/20\n",
      "54000/54000 [==============================] - 27s 505us/step - loss: 0.0096 - acc: 0.9967 - val_loss: 0.0939 - val_acc: 0.9812\n",
      "Epoch 14/20\n",
      "54000/54000 [==============================] - 35s 644us/step - loss: 0.0123 - acc: 0.9963 - val_loss: 0.0953 - val_acc: 0.9812\n",
      "Epoch 15/20\n",
      "54000/54000 [==============================] - 37s 691us/step - loss: 0.0064 - acc: 0.9979 - val_loss: 0.1104 - val_acc: 0.9787\n",
      "Epoch 16/20\n",
      "54000/54000 [==============================] - 28s 527us/step - loss: 0.0118 - acc: 0.9964 - val_loss: 0.1238 - val_acc: 0.9765\n",
      "Epoch 17/20\n",
      "54000/54000 [==============================] - 26s 486us/step - loss: 0.0122 - acc: 0.9962 - val_loss: 0.0951 - val_acc: 0.9823\n",
      "Epoch 18/20\n",
      "54000/54000 [==============================] - 27s 492us/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0856 - val_acc: 0.9842\n",
      "Epoch 19/20\n",
      "54000/54000 [==============================] - 26s 484us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.1130 - val_acc: 0.9827\n",
      "Epoch 20/20\n",
      "54000/54000 [==============================] - 26s 488us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.1011 - val_acc: 0.9842\n",
      "10000/10000 [==============================] - 2s 201us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10260707112895712, 0.9812]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, epochs=num_epochs,\n",
    "          verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "model.evaluate(X_test, Y_test, verbose=1) # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, our model achieves an accuracy of 98.55% on the test set; this is quite respectable for such a simple model, despite being outclassed by state-of-the-art approaches enumerated here.\n",
    "\n",
    "## Attempt different hyperparameter values/optimisation algorithms/activation functions, add more hidden layers, etc. \n",
    "### To achieve accuracies above 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "- 그냥 네우론의 숫자를 늘리는것은 도움이 되지 않는다.\n",
    "예) 512개에서 800개로 늘렸더니 시간이 더 걸리고 accuracy가 98.55% 에서 98.12% 로 줄어듬.\n",
    "- 네우론 수를 512개로 두고 레이어를 하나 더 늘림. 98.24% 나옴.\n",
    "- 베치 사이즈를 두배로 늘리고 3개의 히든 레이어로 하니, 98.13%\n",
    "- training number를 50으로 늘리고, 4개의 레이어을 두니, 98.1%.\n",
    "- training number 50에 layer 6개로 두면서 neuron 갯수를 점점 줄이니 속도는 빨라졌지만 97.86%\n",
    "- training 60번에 배치 두배로 하고 모든 activation을 RELU로 하고 optimization을 ADAMAX로 하니 98.39%. 그나마 최대.\n",
    "- 트레이닐 80까지 올리니 98.52%. 트레이닝 높이면 올라가긴함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,195,018\n",
      "Trainable params: 1,195,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/80\n",
      "54000/54000 [==============================] - 20s 368us/step - loss: 0.2380 - acc: 0.9291 - val_loss: 0.1005 - val_acc: 0.9707\n",
      "Epoch 2/80\n",
      "54000/54000 [==============================] - 19s 345us/step - loss: 0.0988 - acc: 0.9690 - val_loss: 0.0832 - val_acc: 0.9747\n",
      "Epoch 3/80\n",
      "54000/54000 [==============================] - 18s 338us/step - loss: 0.0637 - acc: 0.9805 - val_loss: 0.0815 - val_acc: 0.9727\n",
      "Epoch 4/80\n",
      "54000/54000 [==============================] - 18s 342us/step - loss: 0.0460 - acc: 0.9851 - val_loss: 0.0751 - val_acc: 0.9760\n",
      "Epoch 5/80\n",
      "54000/54000 [==============================] - 19s 359us/step - loss: 0.0330 - acc: 0.9891 - val_loss: 0.0657 - val_acc: 0.9812\n",
      "Epoch 6/80\n",
      "54000/54000 [==============================] - 19s 357us/step - loss: 0.0210 - acc: 0.9935 - val_loss: 0.0743 - val_acc: 0.9803\n",
      "Epoch 7/80\n",
      "54000/54000 [==============================] - 19s 352us/step - loss: 0.0160 - acc: 0.9947 - val_loss: 0.0825 - val_acc: 0.9795\n",
      "Epoch 8/80\n",
      "54000/54000 [==============================] - 20s 363us/step - loss: 0.0119 - acc: 0.9957 - val_loss: 0.0840 - val_acc: 0.9775\n",
      "Epoch 9/80\n",
      "54000/54000 [==============================] - 20s 364us/step - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0784 - val_acc: 0.9800\n",
      "Epoch 10/80\n",
      "54000/54000 [==============================] - 20s 365us/step - loss: 0.0062 - acc: 0.9980 - val_loss: 0.0924 - val_acc: 0.9778\n",
      "Epoch 11/80\n",
      "54000/54000 [==============================] - 20s 369us/step - loss: 0.0066 - acc: 0.9979 - val_loss: 0.0964 - val_acc: 0.9807\n",
      "Epoch 12/80\n",
      "54000/54000 [==============================] - 21s 386us/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0732 - val_acc: 0.9848\n",
      "Epoch 13/80\n",
      "54000/54000 [==============================] - 22s 400us/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0701 - val_acc: 0.9855\n",
      "Epoch 14/80\n",
      "54000/54000 [==============================] - 22s 406us/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0896 - val_acc: 0.9828\n",
      "Epoch 15/80\n",
      "54000/54000 [==============================] - 21s 394us/step - loss: 0.0057 - acc: 0.9980 - val_loss: 0.0789 - val_acc: 0.9845\n",
      "Epoch 16/80\n",
      "54000/54000 [==============================] - 22s 405us/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0793 - val_acc: 0.9847\n",
      "Epoch 17/80\n",
      "54000/54000 [==============================] - 22s 408us/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0817 - val_acc: 0.9847\n",
      "Epoch 18/80\n",
      "54000/54000 [==============================] - 23s 426us/step - loss: 0.0029 - acc: 0.9992 - val_loss: 0.0936 - val_acc: 0.9833\n",
      "Epoch 19/80\n",
      "54000/54000 [==============================] - 22s 409us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0839 - val_acc: 0.9842\n",
      "Epoch 20/80\n",
      "54000/54000 [==============================] - 22s 415us/step - loss: 7.3876e-04 - acc: 0.9997 - val_loss: 0.0843 - val_acc: 0.9843\n",
      "Epoch 21/80\n",
      "54000/54000 [==============================] - 22s 416us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0806 - val_acc: 0.9857\n",
      "Epoch 22/80\n",
      "54000/54000 [==============================] - 23s 430us/step - loss: 2.1583e-04 - acc: 0.9999 - val_loss: 0.0754 - val_acc: 0.9868\n",
      "Epoch 23/80\n",
      "54000/54000 [==============================] - 24s 440us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.1068 - val_acc: 0.9817\n",
      "Epoch 24/80\n",
      "54000/54000 [==============================] - 23s 432us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0969 - val_acc: 0.9835\n",
      "Epoch 25/80\n",
      "54000/54000 [==============================] - 23s 433us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.0816 - val_acc: 0.9855\n",
      "Epoch 26/80\n",
      "54000/54000 [==============================] - 23s 427us/step - loss: 7.3997e-05 - acc: 1.0000 - val_loss: 0.0802 - val_acc: 0.9868\n",
      "Epoch 27/80\n",
      "54000/54000 [==============================] - 24s 439us/step - loss: 1.6018e-05 - acc: 1.0000 - val_loss: 0.0817 - val_acc: 0.9863\n",
      "Epoch 28/80\n",
      "54000/54000 [==============================] - 23s 430us/step - loss: 1.0248e-05 - acc: 1.0000 - val_loss: 0.0829 - val_acc: 0.9863\n",
      "Epoch 29/80\n",
      "54000/54000 [==============================] - 24s 438us/step - loss: 7.1038e-06 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9865\n",
      "Epoch 30/80\n",
      "54000/54000 [==============================] - 24s 440us/step - loss: 4.9381e-06 - acc: 1.0000 - val_loss: 0.0859 - val_acc: 0.9867\n",
      "Epoch 31/80\n",
      "54000/54000 [==============================] - 23s 423us/step - loss: 3.4207e-06 - acc: 1.0000 - val_loss: 0.0875 - val_acc: 0.9867\n",
      "Epoch 32/80\n",
      "54000/54000 [==============================] - 23s 435us/step - loss: 2.3697e-06 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 0.9867\n",
      "Epoch 33/80\n",
      "54000/54000 [==============================] - 23s 419us/step - loss: 1.6152e-06 - acc: 1.0000 - val_loss: 0.0922 - val_acc: 0.9865\n",
      "Epoch 34/80\n",
      "54000/54000 [==============================] - 24s 438us/step - loss: 1.1063e-06 - acc: 1.0000 - val_loss: 0.0937 - val_acc: 0.9863\n",
      "Epoch 35/80\n",
      "54000/54000 [==============================] - 23s 424us/step - loss: 7.7219e-07 - acc: 1.0000 - val_loss: 0.0958 - val_acc: 0.9867\n",
      "Epoch 36/80\n",
      "54000/54000 [==============================] - 23s 426us/step - loss: 5.3398e-07 - acc: 1.0000 - val_loss: 0.0983 - val_acc: 0.9863\n",
      "Epoch 37/80\n",
      "54000/54000 [==============================] - 23s 432us/step - loss: 3.8525e-07 - acc: 1.0000 - val_loss: 0.0995 - val_acc: 0.9865\n",
      "Epoch 38/80\n",
      "54000/54000 [==============================] - 22s 417us/step - loss: 2.8885e-07 - acc: 1.0000 - val_loss: 0.1017 - val_acc: 0.9865\n",
      "Epoch 39/80\n",
      "54000/54000 [==============================] - 22s 415us/step - loss: 2.2562e-07 - acc: 1.0000 - val_loss: 0.1036 - val_acc: 0.9865\n",
      "Epoch 40/80\n",
      "54000/54000 [==============================] - 23s 421us/step - loss: 1.8493e-07 - acc: 1.0000 - val_loss: 0.1061 - val_acc: 0.9863\n",
      "Epoch 41/80\n",
      "54000/54000 [==============================] - 22s 406us/step - loss: 1.6036e-07 - acc: 1.0000 - val_loss: 0.1080 - val_acc: 0.9867\n",
      "Epoch 42/80\n",
      "54000/54000 [==============================] - 23s 417us/step - loss: 1.4478e-07 - acc: 1.0000 - val_loss: 0.1096 - val_acc: 0.9865\n",
      "Epoch 43/80\n",
      "54000/54000 [==============================] - 24s 440us/step - loss: 1.3445e-07 - acc: 1.0000 - val_loss: 0.1119 - val_acc: 0.9865\n",
      "Epoch 44/80\n",
      "54000/54000 [==============================] - 22s 413us/step - loss: 1.2865e-07 - acc: 1.0000 - val_loss: 0.1128 - val_acc: 0.9862\n",
      "Epoch 45/80\n",
      "54000/54000 [==============================] - 22s 413us/step - loss: 1.2508e-07 - acc: 1.0000 - val_loss: 0.1143 - val_acc: 0.9863\n",
      "Epoch 46/80\n",
      "54000/54000 [==============================] - 22s 411us/step - loss: 1.2245e-07 - acc: 1.0000 - val_loss: 0.1172 - val_acc: 0.9865\n",
      "Epoch 47/80\n",
      "54000/54000 [==============================] - 23s 419us/step - loss: 1.2113e-07 - acc: 1.0000 - val_loss: 0.1176 - val_acc: 0.9865\n",
      "Epoch 48/80\n",
      "54000/54000 [==============================] - 22s 412us/step - loss: 1.2024e-07 - acc: 1.0000 - val_loss: 0.1187 - val_acc: 0.9863\n",
      "Epoch 49/80\n",
      "54000/54000 [==============================] - 22s 403us/step - loss: 1.1976e-07 - acc: 1.0000 - val_loss: 0.1194 - val_acc: 0.9865\n",
      "Epoch 50/80\n",
      "54000/54000 [==============================] - 22s 400us/step - loss: 1.1949e-07 - acc: 1.0000 - val_loss: 0.1205 - val_acc: 0.9865\n",
      "Epoch 51/80\n",
      "54000/54000 [==============================] - 23s 420us/step - loss: 1.1934e-07 - acc: 1.0000 - val_loss: 0.1209 - val_acc: 0.9865\n",
      "Epoch 52/80\n",
      "54000/54000 [==============================] - 22s 416us/step - loss: 1.1926e-07 - acc: 1.0000 - val_loss: 0.1212 - val_acc: 0.9865\n",
      "Epoch 53/80\n",
      "54000/54000 [==============================] - 22s 407us/step - loss: 1.1923e-07 - acc: 1.0000 - val_loss: 0.1216 - val_acc: 0.9865\n",
      "Epoch 54/80\n",
      "54000/54000 [==============================] - 22s 406us/step - loss: 1.1922e-07 - acc: 1.0000 - val_loss: 0.1216 - val_acc: 0.9865\n",
      "Epoch 55/80\n",
      "54000/54000 [==============================] - 22s 406us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1220 - val_acc: 0.9868\n",
      "Epoch 56/80\n",
      "54000/54000 [==============================] - 23s 432us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1224 - val_acc: 0.9865\n",
      "Epoch 57/80\n",
      "54000/54000 [==============================] - 22s 408us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1226 - val_acc: 0.9867\n",
      "Epoch 58/80\n",
      "54000/54000 [==============================] - 22s 411us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1224 - val_acc: 0.9865\n",
      "Epoch 59/80\n",
      "54000/54000 [==============================] - 22s 409us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1226 - val_acc: 0.9865\n",
      "Epoch 60/80\n",
      "54000/54000 [==============================] - 22s 410us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1231 - val_acc: 0.9867\n",
      "Epoch 61/80\n",
      "54000/54000 [==============================] - 22s 403us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 62/80\n",
      "54000/54000 [==============================] - 22s 412us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1232 - val_acc: 0.9865\n",
      "Epoch 63/80\n",
      "54000/54000 [==============================] - 22s 415us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 64/80\n",
      "54000/54000 [==============================] - 23s 418us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1231 - val_acc: 0.9865\n",
      "Epoch 65/80\n",
      "54000/54000 [==============================] - 22s 416us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1234 - val_acc: 0.9865\n",
      "Epoch 66/80\n",
      "54000/54000 [==============================] - 22s 407us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 67/80\n",
      "54000/54000 [==============================] - 22s 415us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 68/80\n",
      "54000/54000 [==============================] - 22s 414us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 69/80\n",
      "54000/54000 [==============================] - 23s 417us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 70/80\n",
      "54000/54000 [==============================] - 23s 430us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 71/80\n",
      "54000/54000 [==============================] - 23s 422us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 72/80\n",
      "54000/54000 [==============================] - 22s 412us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 73/80\n",
      "54000/54000 [==============================] - 22s 416us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 74/80\n",
      "54000/54000 [==============================] - 24s 438us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 75/80\n",
      "54000/54000 [==============================] - 22s 406us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 76/80\n",
      "54000/54000 [==============================] - 23s 417us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 77/80\n",
      "54000/54000 [==============================] - 22s 412us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 78/80\n",
      "54000/54000 [==============================] - 22s 411us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 79/80\n",
      "54000/54000 [==============================] - 23s 422us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "Epoch 80/80\n",
      "54000/54000 [==============================] - 22s 409us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9865\n",
      "10000/10000 [==============================] - 2s 200us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12867327174819607, 0.9852]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128 # in each iteration, we consider 128 training examples at once\n",
    "num_epochs = 80 # we iterate twenty times over the entire training set\n",
    "hidden_size = 512 # there will be 512 neurons in both hidden layers \n",
    "\n",
    "\n",
    "inp = Input(shape=(height * width,)) # Our input is a 1D vector of size 784 (= 28*28)\n",
    "hidden_1 = Dense(hidden_size, activation='relu')(inp) # First hidden ReLU layer\n",
    "hidden_2 = Dense(hidden_size, activation='relu')(hidden_1) # Second hidden ReLU layer\n",
    "hidden_3 = Dense(hidden_size, activation='relu')(hidden_2) # Third hidden ReLU layer\n",
    "hidden_4 = Dense(hidden_size, activation='relu')(hidden_3) # Fourth hidden ReLU layer\n",
    "\n",
    "out = Dense(num_classes, activation='softmax')(hidden_4) # Output softmax layer\n",
    "\n",
    "model = Model(inputs=inp, outputs=out) # To define a model, just specify its input and output layers\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='Adamax', # using the Adam optimiser\n",
    "              metrics=['accuracy']) # reporting the accuracy\n",
    "model.fit(X_train, Y_train, # Train the model using the training set...\n",
    "          batch_size=batch_size, epochs=num_epochs,\n",
    "          verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation\n",
    "model.evaluate(X_test, Y_test, verbose=1) # Evaluate the trained model on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
